# -*- coding: utf-8 -*-
"""backprop.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BSXm8XZ0N2M7IteNpAhk8mG9ywg9bOO6
"""

import numpy as np

W1 = np.ones((4,6))
W2 = np.ones((6,3))
b1 = np.ones((1,6))
b2 = np.ones((1,3))

def sigmoid(x):
    return 1/(1+np.exp(-x))


def feedforward(X):
  
    z1 = np.dot(X,W1) + b1
    a1 = sigmoid(z1)
    
    
    z2 = np.dot(a1,W2) + b2
    a2 = sigmoid(z2)
    
    return a2

def sum_of_squares_loss(y, t):
    return 0.5*np.sum(np.power(y-t, 2))

def backpropagation(X, t, y, a1, a2, W1, W2, b1, b2, lr):
    delta2 = (y-t)*a2*(1-a2)
    
    delta1 = np.dot(delta2, W2.T)*a1*(1-a1)
    
    
    W2 -= lr*np.dot(a1.T, delta2)
    b2 -= lr*np.sum(delta2, axis=0)
    W1 -= lr*np.dot(X.T, delta1)
    b1 -= lr*np.sum(delta1, axis=0)
    
    return W1, W2, b1, b2

    

input_values = [float(input()) for _ in range(7)]
X = np.array(input_values[:4]).reshape(1,4)
t = np.array(input_values[4:]).reshape(1,3)

y = feedforward(X)
loss = sum_of_squares_loss(y, t)


print("%.4f" % loss)

a1 = sigmoid(np.dot(X, W1) + b1)
a2 = sigmoid(np.dot(a1, W2) + b2)
W1, W2, b1, b2 = backpropagation(X, t, a2, a1, y, W1, W2, b1, b2, lr=0.1)


y_new = feedforward(X)
loss_new = sum_of_squares_loss(y_new, t)


print( "%.4f" % loss_new)